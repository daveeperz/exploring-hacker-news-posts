{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring Hacker News Posts\n",
    "[Hacker News](https://news.ycombinator.com/) is a website where technology articles, ideas, and related subjects are shared. Users share stories known as \"posts\", which are able to receive votes and comments, similar to other discussion forums like reddit. It is extremely popular within startup and technology circles. Posts that get upvoted enough to make it to the top of the Hacker News listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "## Exploring the Data\n",
    "The [dataset](https://www.kaggle.com/datasets/hacker-news/hacker-news-posts) that will be used contains about 300,000 rows. However, the [resulting dataset](https://dq-content.s3.amazonaws.com/356/hacker_news.csv) has been reduced to approximately 20,000 rows by removing all submissions that did not receive any comments and then randomly sampling  from the remaining submissions.\n",
    "\n",
    "Below are descriptions of the columns:\n",
    "\n",
    "- `id`: the unique identifier from Hacker News for the post\n",
    "- `title`: the title of the post\n",
    "- `url`: the URL that the posts links to, if the post has a URL\n",
    "- `num_points`: the number of points the post acquired, calculated as the total number of upvotes minus - the total number of downvotes\n",
    "- `num_comments`: the number of comments on the post\n",
    "- `author`: the username of the person who submitted the post\n",
    "- `created_at`: the date and time of the post's submission\n",
    "\n",
    "Let's start by opening the dataset and displaying the first results. For that, we will need two functions: \n",
    "- `open_dataset`, whose work is to open the dataset provided and decide wether we want the function to consider returning a header or not.\n",
    "- `explore_dataset` formats a section of a selected dataset for easier visualization. It also provides options to display the total amount of rows and columns in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "\n",
      "\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "\n",
      "\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "\n",
      "\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "\n",
      "\n",
      "['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']\n",
      "\n",
      "\n",
      "Number of columns:  7\n",
      "Number of rows:  20,100\n"
     ]
    }
   ],
   "source": [
    "# Open a csv dataset and opt to return the dataset as a whole or separated by body and header\n",
    "def open_dataset(file_name = \"hacker_news.csv\", header = True):\n",
    "    from csv import reader\n",
    "    opened_file = open(file_name)\n",
    "    read_file = reader(opened_file)\n",
    "    dataset = list(read_file)\n",
    "    \n",
    "    if header == False: \n",
    "        return dataset\n",
    "    else:\n",
    "        header = dataset[0]\n",
    "        body = dataset[1:]\n",
    "                   \n",
    "        return header, body\n",
    "\n",
    "# Explore a certain section of the dataset and opt to show the total count of rows and columns\n",
    "def explore_dataset(dataset, start, end, rows_and_columns =  False):\n",
    "    for row in dataset[start:end]:\n",
    "        print(row)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    if rows_and_columns:\n",
    "        columns = len(dataset[0])\n",
    "        rows = \"{:,d}\".format(len(dataset))\n",
    "        print(\"Number of columns: \", columns)\n",
    "        print(\"Number of rows: \", rows)\n",
    "        \n",
    "headers, hn = open_dataset()\n",
    "explore_dataset(hn, 0, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the headers contained within the dataset. They have been previously approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n"
     ]
    }
   ],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Ask HN and Show HN Posts\n",
    "After opening the data, we need to classify Ask HN and Show HN posts into two different lists. This will allow us to work efficiently when exploring and manipulating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ask HN posts:  1744\n",
      "Number of Show HN posts:  1162\n",
      "Number of Other HN posts:  17194\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    \n",
    "    if title.lower().startswith(\"ask hn\"):\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith(\"show hn\"):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "\n",
    "print(\"Number of Ask HN posts: \", len(ask_posts))        \n",
    "print(\"Number of Show HN posts: \", len(show_posts))\n",
    "print(\"Number of Other HN posts: \", len(other_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Average Number of Comments for Ask HN and Show HN Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each post in the website has a certain amount of interactions (comments) done to a certain post. Let us find out what is the average number of comments for Ask HN and Show HN posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Ask HN comments:  14.038417431192661\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    total_ask_comments += int(row[4])\n",
    "\n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "print(\"Average Ask HN comments: \", avg_ask_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Show HN comments:  10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_show_comments = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    total_show_comments += int(row[4])\n",
    "    \n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "print(\"Average Show HN comments: \", avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the calculations performed to Show HN and Ask HN posts, it is acceptable to conclude that the average number of interactions, in other words \"comments\" posted on Ask HN posts is higher than those published on Show HN posts. Ask HN has resulted to have an average of fourteen comments per post while Show HN has an average of 10 comments per post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Number of Ask Posts and Comments by Hour Created \n",
    " Givcen the previous results, we will be focusing only in the Ask HN post, as these posts prevail as the ones with more interactions. Now, let us determine if ask posts created at a certain time are more likely to attract comments. For this, we will need to perform a couple of tasks:\n",
    " 1. Calculate the number of ask posts created in each hour of the day, along with the number of comments received.\n",
    " 2. Calculate the average number of comments ask posts receive by hour created.\n",
    " \n",
    "Let's proceed with the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    num_comments = row[4]\n",
    "    result_list.append([created_at, num_comments])\n",
    "    \n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    num_comments = int(row[1])\n",
    "    date_format = \"%m/%d/%Y %H:%M\"\n",
    "    datetime_obj = dt.datetime.strptime(date, date_format)\n",
    "    hour = datetime_obj.strftime(\"%H\")\n",
    "    \n",
    "    if hour not in counts_by_hour:\n",
    "        counts_by_hour[hour] = 1\n",
    "        comments_by_hour[hour] = num_comments\n",
    "    else:\n",
    "        counts_by_hour[hour] += 1\n",
    "        comments_by_hour[hour] += num_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Average Number of Comments for Ask HN Posts by Hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously produced dictionaries are defined as follows:\n",
    "- `counts_by_hour`: Contains the number of ask posts created during each hour of the day.\n",
    "- `comments_by_hour`: Contains the corresponding number of commetns ask posts created at each hour received.\n",
    "\n",
    "We will now calculate the average number of comments that interacted with each post on every hour. The result will be stored in a list of lists called `avg_by_hour`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['09', 5.5777777777777775], ['13', 14.741176470588234], ['10', 13.440677966101696], ['14', 13.233644859813085], ['16', 16.796296296296298], ['23', 7.985294117647059], ['12', 9.41095890410959], ['17', 11.46], ['15', 38.5948275862069], ['21', 16.009174311926607], ['20', 21.525], ['02', 23.810344827586206], ['18', 13.20183486238532], ['03', 7.796296296296297], ['05', 10.08695652173913], ['19', 10.8], ['01', 11.383333333333333], ['22', 6.746478873239437], ['08', 10.25], ['04', 7.170212765957447], ['00', 8.127272727272727], ['06', 9.022727272727273], ['07', 7.852941176470588], ['11', 11.051724137931034]]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append([hour, comments_by_hour[hour] / counts_by_hour[hour]])\n",
    "    \n",
    "print(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Printing Values from a List of Lists\n",
    "It is important to format the valaues of the `avg_by_hour` list, allowing for a better readability of the iterable. Let us swap the order of the columns within the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.5777777777777775, '09'], [14.741176470588234, '13'], [13.440677966101696, '10'], [13.233644859813085, '14'], [16.796296296296298, '16'], [7.985294117647059, '23'], [9.41095890410959, '12'], [11.46, '17'], [38.5948275862069, '15'], [16.009174311926607, '21'], [21.525, '20'], [23.810344827586206, '02'], [13.20183486238532, '18'], [7.796296296296297, '03'], [10.08695652173913, '05'], [10.8, '19'], [11.383333333333333, '01'], [6.746478873239437, '22'], [10.25, '08'], [7.170212765957447, '04'], [8.127272727272727, '00'], [9.022727272727273, '06'], [7.852941176470588, '07'], [11.051724137931034, '11']]\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1], row[0]])\n",
    "    \n",
    "print(swap_avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after swapping the columns, we may proceed to make a recommendation after formatting the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hours for Ask Posts Comments\n",
      "15:00: 38.59 average comments per post\n",
      "02:00: 23.81 average comments per post\n",
      "20:00: 21.52 average comments per post\n",
      "16:00: 16.80 average comments per post\n",
      "21:00: 16.01 average comments per post\n"
     ]
    }
   ],
   "source": [
    "sorted_swap = sorted(swap_avg_by_hour, reverse =  True)\n",
    "print(\"Top 5 hours for Ask Posts Comments\")\n",
    "string = \"{}: {:.2f} average comments per post\"\n",
    "\n",
    "for row in sorted_swap[:5]:\n",
    "    avg_comments_by_hour = row[0]\n",
    "    time = row[1]\n",
    "    time = dt.datetime.strptime(time, \"%H\")\n",
    "    formatted_time = time.strftime(\"%H:%M\")\n",
    "    result_string = string.format(formatted_time, avg_comments_by_hour)\n",
    "    print(result_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 timings depict times at which the chance of receiving a major amount of comments becomes higher. If a post writer was to be looking for the best time stamps to make a post, these would be his best options.\n",
    "\n",
    "## Converting Time Stamps Between Time Zones\n",
    "Let us explore another scenario. Remembering that the time zone of the dates the dataset contains was US' Eastern Time, what would be the appropriate time for me to make a post and have a higher chance of receiving comments? This is of course converting the time stamps to my time zone: Mexico Central Standard Time (CST/UTC-6). \n",
    "\n",
    "Normally, this would be an appropriate scenario to work witht the `zoneinfo` module, however, the action won't be repeated, therefore, can be done by hand as a one time activity. These are the converted times and best time options for a person located in the CST time zone to make a post in Hacker News:\n",
    "- 15:00\n",
    "- 04:00\n",
    "- 22:00\n",
    "- 18:00\n",
    "- 23:00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The Hacker News dataset provides excellent opportunities for exploration, nevertheless, it all depends on the needs and goals of the explorer. It is reasonable to accept that the researched and concluded time zones produce better opportunities for Hacker News post writers looking for highere chances of feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
